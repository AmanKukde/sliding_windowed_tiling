{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things we need further down\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pooch\n",
    "import matplotlib.pyplot as plt\n",
    "from careamics.lightning import VAEModule\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import plot_input_patches\n",
    "\n",
    "# Dataset specific imports...\n",
    "from microsplit_reproducibility.configs.parameters.custom_dataset_2D import (\n",
    "    get_microsplit_parameters\n",
    ")\n",
    "from microsplit_reproducibility.configs.data.custom_dataset_2D import get_data_configs\n",
    "from microsplit_reproducibility.datasets.custom_dataset_2D import get_train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = pooch.create(\n",
    "#     path=f\"./data/\",\n",
    "#     base_url=f\"https://download.fht.org/jug/msplit/ht_lif24/data_tiff/\",\n",
    "#     registry={f\"ht_lif24_5ms_reduced.zip\": None},\n",
    "# )\n",
    "# for fname in DATA.registry:\n",
    "#     DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)\n",
    "\n",
    "# DATA_PATH = DATA.abspath / (DATA.registry_files[0] + \".unzip/5ms/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR set the path to your own data\n",
    "Important: the path should end with `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ROOT = Path(\"/group/jug/aman/Datasets/PAVIA_ATN/\")\n",
    "DATA_PATH = RESULTS_ROOT/ \"data\"\n",
    "NM_PATH = RESULTS_ROOT/\"noise_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the path to the noise models\n",
    "This is the path to the noise models that you trained in the notebook **00_noisemodels.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we load the image data to be processed\n",
    "\n",
    "***Note*** that depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> below.\n",
    "\n",
    "Number of epochs is set to 10, which usually allows to see decent results. However, for getting optimal performance you can increase it to 50.\n",
    "\n",
    "You also need to change the `num_channels` and `target_channels` parameters respectively for the `get_data_configs()` and the `get_microsplit_parameters` functions. These parameters have a similar meaning, i.e., they control the number of channels in the input data depending on how many channels you want to split.\n",
    "\n",
    "Finally, ensure that the `image_size` parameter (i.e., the patch size in (`Z`, `Y`, `X`) you want to use to train the model) is properly set given the size of your data and of you GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 2\n",
    "\"\"\"The number of channels considered for the splitting task.\"\"\"\n",
    "BATCH_SIZE = 32*8\n",
    "\"\"\"The batch size for training.\"\"\"\n",
    "PATCH_SIZE = (64, 64)\n",
    "\"\"\"The size of the patches fed to the network for training in (Y, X).\"\"\"\n",
    "EPOCHS = 50\n",
    "\"\"\"The number of epochs to train the network.\"\"\"\n",
    "\n",
    "assert len(PATCH_SIZE) == 2, \"PATCH_SIZE must be a tuple of length 2 (Y, X) since we are using 2D data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataclass is <class 'careamics.lvae_training.dataset.lc_dataset.LCMultiChDloader'>\n",
      "\n",
      "Padding is not used with this alignement style\n",
      "\n",
      "Padding is not used with this alignement style\n",
      "\n",
      "Padding is not used with this alignement style\n"
     ]
    }
   ],
   "source": [
    "# setting up train, validation, and test data configs\n",
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=PATCH_SIZE,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    # sliding_window_flag=True,\n",
    ")\n",
    "\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = get_microsplit_parameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    img_size=PATCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    multiscale_count=3,\n",
    "    noise_model_path=NM_PATH,\n",
    "    target_channels=NUM_CHANNELS,\n",
    ")\n",
    "\n",
    "# create the dataset\n",
    "train_dset, val_dset, test_dset, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA_PATH,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure `num_workers`\n",
    "In Windows and MacOS, setting `num_workers > 0` for dataloaders would cause out-of-memory issue and might crash the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_workers():\n",
    "    \"\"\"Utility function to set num_workers based on OS.\"\"\"\n",
    "    if platform.system() == \"Windows\" or platform.system() == \"Darwin\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 3  # or any other number suitable for your system\n",
    "\n",
    "experiment_params[\"num_workers\"] = get_num_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional:*** inspect data configurations and <nobr>Micro$\\mathbb{S}$plit</nobr> config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FYI: train_data_config\n",
      "----------------------\n",
      "('data_type', <DataType.HTLIF24Data: 1>)\n",
      "('depth3D', 1)\n",
      "('datasplit_type', <DataSplitType.Train: 1>)\n",
      "('num_channels', 2)\n",
      "('ch1_fname', None)\n",
      "('ch2_fname', None)\n",
      "('ch_input_fname', None)\n",
      "('input_is_sum', False)\n",
      "('input_idx', None)\n",
      "('target_idx_list', None)\n",
      "('start_alpha', None)\n",
      "('end_alpha', None)\n",
      "('image_size', (64, 64))\n",
      "('grid_size', 32)\n",
      "('empty_patch_replacement_enabled', False)\n",
      "('empty_patch_replacement_channel_idx', None)\n",
      "('empty_patch_replacement_probab', None)\n",
      "('empty_patch_max_val_threshold', None)\n",
      "('uncorrelated_channels', False)\n",
      "('uncorrelated_channel_probab', 0.5)\n",
      "('poisson_noise_factor', -1.0)\n",
      "('synthetic_gaussian_scale', 100.0)\n",
      "('input_has_dependant_noise', True)\n",
      "('enable_gaussian_noise', False)\n",
      "('allow_generation', False)\n",
      "('training_validtarget_fraction', None)\n",
      "('deterministic_grid', None)\n",
      "('enable_rotation_aug', False)\n",
      "('max_val', None)\n",
      "('overlapping_padding_kwargs', {'mode': 'reflect'})\n",
      "('print_vars', False)\n",
      "('normalized_input', True)\n",
      "('use_one_mu_std', True)\n",
      "('train_aug_rotate', True)\n",
      "('enable_random_cropping', True)\n",
      "('multiscale_lowres_count', 3)\n",
      "('tiling_mode', <TilingMode.ShiftBoundary: 2>)\n",
      "('target_separate_normalization', True)\n",
      "('mode_3D', False)\n",
      "('trainig_datausage_fraction', 1.0)\n",
      "('validtarget_random_fraction', None)\n",
      "('validation_datausage_fraction', 1.0)\n",
      "('random_flip_z_3D', False)\n",
      "('padding_kwargs', {'mode': 'reflect'})\n",
      "('sliding_window_flag', False)\n",
      "\n",
      "FYI: experiment_params\n",
      "----------------------\n",
      "{'algorithm': 'denoisplit', 'loss_type': 'denoisplit_musplit', 'img_size': (64, 64), 'encoder_conv_strides': [2, 2], 'decoder_conv_strides': [2, 2], 'target_channels': 2, 'multiscale_count': 3, 'predict_logvar': 'pixelwise', 'nm_paths': ['noise_models/noise_model_Ch0.npz', 'noise_models/noise_model_Ch1.npz'], 'kl_type': 'kl_restricted', 'batch_size': 256, 'lr': 0.001, 'lr_scheduler_patience': 10, 'earlystop_patience': 200, 'num_epochs': 50, 'num_workers': 3, 'mmse_count': 50, 'grid_size': 32}\n"
     ]
    }
   ],
   "source": [
    "do_show_configs = True\n",
    "\n",
    "if do_show_configs:\n",
    "    print(\"FYI: train_data_config\")\n",
    "    print(\"----------------------\")\n",
    "    for cfg in train_data_config:\n",
    "        print(cfg)\n",
    "\n",
    "    print(\"\\nFYI: experiment_params\")\n",
    "    print(\"----------------------\")\n",
    "    print(experiment_params)\n",
    "else:\n",
    "    print(\"You opted out of having all params printed... swiftly moving on... ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: create Dataloaders for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.2:** Prepare <nobr>Micro$\\mathbb{S}$plit</nobr> Training\n",
    "Next, we create all the configs for the upcoming network training run. These lines are not very intuitive and if you don't intend to dive really deep into CAREamics and the internals of <nobr>Micro$\\mathbb{S}$plit</nobr>, you might just execute these cells and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman.kukde/Projects_2/careamics/src/careamics/utils/serializers.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lst)\n"
     ]
    }
   ],
   "source": [
    "# making our data_stas known to the experiment we prepare\n",
    "experiment_params[\"data_stats\"] = data_stats\n",
    "\n",
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the <nobr>Micro$\\mathbb{S}$plit</nobr> model to be trained.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianMixtureNoiseModel] min_sigma: 200.0\n",
      "[GaussianMixtureNoiseModel] min_sigma: 200.0\n",
      "[MultiChannelNoiseModel] Nmodels count:2\n",
      "[GaussianLikelihood] PredLVar:pixelwise LowBLVar:-5.0\n"
     ]
    }
   ],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load checkpoint (optional and for you to implement)*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> If you would like to continue a previous training run or finetune a compatible pre-trained model, here would be a good place. You will need to figure out how to implement this for your use-case, but to give you a head-start, we left three potentially useful lines of code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from microsplit_reproducibility.notebook_utils.HT_LIF24 import load_pretrained_model\n",
    "# ckpt_path = load_checkpoint_path(f\"./pretrained_checkpoints/{EXPOSURE_DURATION}/\", best=True)\n",
    "# load_pretrained_model(model, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training data for a final check!\n",
    "***Tip:*** we show you a few samples of the prepared training data. In case you don't like what you see, execute the cell again and other randomly chosen patches will be shown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=NUM_CHANNELS, num_samples=3, patch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.3:** Train the prepared model!\n",
    "***Note:*** if this takes too long, there were to places above where we gave you options to *(i)* reduce the amount of training data, and *(ii)* chose to train for fewer epochs. Revisit your choices if you want to!\n",
    "\n",
    "***Note:*** Depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(RESULTS_ROOT/\"checkpoints/\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "# start the training\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.4:** Predict and visualize results for validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, reduce the validation dataset to speed up the evaluation\n",
    "val_dset.reduce_data([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note*** Parameter `mmse_count` is responsible for how many samples are generated for each patch. The default value is 1, but in this case you might see stitching artifacts because each patch will be slightly different. You can increase this value to 10 to get a smoother image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    get_unnormalized_predictions,\n",
    "    get_target,\n",
    "    get_input,\n",
    ")\n",
    "\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(\n",
    "    model,\n",
    "    test_dset,\n",
    "    data_key=val_dset._fpath.name,\n",
    "    mmse_count=experiment_params['mmse_count'],\n",
    "    num_workers=0,\n",
    "    batch_size=8\n",
    ")\n",
    "tar = get_target(test_dset)\n",
    "\n",
    "# get input as sum of the two channels\n",
    "inp = get_input(test_dset).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill\n",
    "save_dir = RESULTS_ROOT/'predictions'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "# saving the predictions as a tiff file\n",
    "stitched_preds_np = np.array(stitched_predictions)\n",
    "print(\"Shape before squeeze:\", stitched_preds_np.shape)\n",
    "\n",
    "with open(save_dir / \"pred_test_dset_microsplit_og.pkl\", \"wb\") as f:\n",
    "    dill.dump(stitched_preds_np, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: visualize predictions on validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "ax[0, 0].imshow(tar[0, ..., 0], cmap=\"gray\")\n",
    "ax[0, 0].set_title(\"Input ch1\")\n",
    "ax[0, 1].imshow(tar[0, ..., 1], cmap=\"gray\")\n",
    "ax[0, 1].set_title(\"Input ch2\")\n",
    "ax[1, 0].imshow(stitched_predictions[0, ..., 0], cmap=\"gray\")\n",
    "ax[1, 0].set_title(\"Prediction ch1\")\n",
    "ax[1, 1].imshow(stitched_predictions[0, ..., 1], cmap=\"gray\")\n",
    "ax[1, 1].set_title(\"Prediction ch2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import full_frame_evaluation\n",
    "\n",
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds\"\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed view on some (foreground) locations...\n",
    "Below, we show few random foreground locations and the corresponding <nobr>Micro$\\mathbb{S}$plit</nobr> predictions.\n",
    "\n",
    "As before, also here you can execute the cell multiple times and different randomly chosen locations will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from microsplit_reproducibility.utils.utils import clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    pick_random_patches_with_content,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sz = 128\n",
    "rand_locations = pick_random_patches_with_content(tar, 128)\n",
    "h_start = rand_locations[\n",
    "    2, 0\n",
    "]  # np.random.randint(stitched_predictions.shape[1] - img_sz)\n",
    "w_start = rand_locations[\n",
    "    2, 1\n",
    "]  # np.random.randint(stitched_predictions.shape[2] - img_sz)\n",
    "\n",
    "ncols = 4 + 1\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i, 0].imshow(inp[0, h_start : h_start + img_sz, w_start : w_start + img_sz])\n",
    "    for j in range(ncols // 2):\n",
    "        vmin = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].min()\n",
    "        vmax = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].max()\n",
    "        ax[i, 2 * j + 1].imshow(\n",
    "            tar[0, h_start : h_start + img_sz, w_start : w_start + img_sz, j],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax[i, 2 * j + 2].imshow(\n",
    "            stitched_predictions[\n",
    "                0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "            ],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "\n",
    "ax[0, 0].set_title(\"Primary Input\")\n",
    "for i in range(2):\n",
    "    ax[0, 2 * i + 1].set_title(f\"Target Channel {i+1}\")\n",
    "    ax[0, 2 * i + 2].set_title(f\"Predicted Channel {i+1}\")\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* manual inspection of the predictions\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Task:</b> Set <i>y_start</i>, <i>x_start</i>, and <i>crop_size</i> to inspect the predictions at a  location of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_start = 600  # np.random.randint(stitched_predictions.shape[1] - crop_size)\n",
    "x_start = 1150  # np.random.randint(stitched_predictions.shape[2] - crop_size)\n",
    "crop_size = 128\n",
    "\n",
    "ncols = 3\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "ax[0, 0].imshow(inp[0, y_start : y_start + crop_size, x_start : x_start + crop_size])\n",
    "for i in range(ncols - 1):\n",
    "    vmin = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].min()\n",
    "    vmax = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].max()\n",
    "    ax[0, i + 1].imshow(\n",
    "        tar[0, y_start : y_start + crop_size, x_start : x_start + crop_size, i],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    ax[1, i + 1].imshow(\n",
    "        stitched_predictions[\n",
    "            0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "        ],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[1, 0].axis(\"off\")\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "ax[0, 1].set_title(\"Channel 1\")\n",
    "ax[0, 2].set_title(\"Channel 2\")\n",
    "# set y labels on the right for ax[0,2]\n",
    "ax[0, 2].yaxis.set_label_position(\"right\")\n",
    "ax[0, 2].set_ylabel(\"Target\")\n",
    "\n",
    "ax[1, 2].yaxis.set_label_position(\"right\")\n",
    "ax[1, 2].set_ylabel(\"Predicted\")\n",
    "\n",
    "print(\"Here the crop you selected:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Optional Step 1.4:*** Posterior Sampling\n",
    "For a given input patch, <nobr>Micro$\\mathbb{S}$plit</nobr> can generate multiple outputs. This is possible because <nobr>Micro$\\mathbb{S}$plit</nobr> is learning a full posterior of possible solutions, which is a quite powerful feature!\n",
    "\n",
    "As we elaborate in the <nobr>Micro$\\mathbb{S}$plit</nobr> paper and also later in the calibration notebook `03_calibration.ipynb`, this allows users to visually judge and even quantify the (data) uncertainty in the predictions their trained model makes.\n",
    "\n",
    "Below, we show two posterior samples and how much they differ for a few random foreground locations. Re-run the cell to see different randomly choosen locations and corresponding posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import show_sampling\n",
    "\n",
    "imgsz = 3\n",
    "ncols = 6\n",
    "examplecount = 3\n",
    "_, ax = plt.subplots(\n",
    "    figsize=(imgsz * ncols, imgsz * 2 * examplecount),\n",
    "    ncols=ncols,\n",
    "    nrows=2 * examplecount,\n",
    ")\n",
    "\n",
    "show_sampling(val_dset, model, ax=ax[:2])\n",
    "show_sampling(val_dset, model, ax=ax[2:4])\n",
    "show_sampling(val_dset, model, ax=ax[4:6])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done here! üëç Congratulations! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
