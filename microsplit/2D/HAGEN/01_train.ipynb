{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things we need further down\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "import pooch\n",
    "import matplotlib.pyplot as plt\n",
    "from careamics.lightning import VAEModule\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import plot_input_patches\n",
    "\n",
    "# Dataset specific imports...\n",
    "from microsplit_reproducibility.configs.parameters.custom_dataset_2D import (\n",
    "    get_microsplit_parameters\n",
    ")\n",
    "from microsplit_reproducibility.configs.data.custom_dataset_2D import get_data_configs\n",
    "from microsplit_reproducibility.datasets.custom_dataset_2D import get_train_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = pooch.create(\n",
    "#     path=f\"./data/\",\n",
    "#     base_url=f\"https://download.fht.org/jug/msplit/ht_lif24/data_tiff/\",\n",
    "#     registry={f\"ht_lif24_5ms_reduced.zip\": None},\n",
    "# )\n",
    "# for fname in DATA.registry:\n",
    "#     DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)\n",
    "\n",
    "# DATA_PATH = DATA.abspath / (DATA.registry_files[0] + \".unzip/5ms/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR set the path to your own data\n",
    "Important: the path should end with `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/group/jug/aman/Datasets/Hagen/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the path to the noise models\n",
    "This is the path to the noise models that you trained in the notebook **00_noisemodels.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NM_PATH = Path(\"./noise_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we load the image data to be processed\n",
    "\n",
    "***Note*** that depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> below.\n",
    "\n",
    "Number of epochs is set to 10, which usually allows to see decent results. However, for getting optimal performance you can increase it to 50.\n",
    "\n",
    "You also need to change the `num_channels` and `target_channels` parameters respectively for the `get_data_configs()` and the `get_microsplit_parameters` functions. These parameters have a similar meaning, i.e., they control the number of channels in the input data depending on how many channels you want to split.\n",
    "\n",
    "Finally, ensure that the `image_size` parameter (i.e., the patch size in (`Z`, `Y`, `X`) you want to use to train the model) is properly set given the size of your data and of you GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 2\n",
    "\"\"\"The number of channels considered for the splitting task.\"\"\"\n",
    "BATCH_SIZE = 32*8\n",
    "\"\"\"The batch size for training.\"\"\"\n",
    "PATCH_SIZE = (64, 64)\n",
    "\"\"\"The size of the patches fed to the network for training in (Y, X).\"\"\"\n",
    "EPOCHS = 50\n",
    "\"\"\"The number of epochs to train the network.\"\"\"\n",
    "\n",
    "assert len(PATCH_SIZE) == 2, \"PATCH_SIZE must be a tuple of length 2 (Y, X) since we are using 2D data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataclass is <class 'careamics.lvae_training.dataset.lc_dataset.LCMultiChDloader'>\n",
      "\n",
      "Padding is not used with this alignement style\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m experiment_params \u001b[38;5;241m=\u001b[39m get_microsplit_parameters(\n\u001b[1;32m      9\u001b[0m     algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdenoisplit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     img_size\u001b[38;5;241m=\u001b[39mPATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     target_channels\u001b[38;5;241m=\u001b[39mNUM_CHANNELS,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# create the dataset\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_dset, val_dset, _, data_stats \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_val_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatapath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_data_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_train_val_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects_2/MicroSplit-reproducibility/src/microsplit_reproducibility/datasets/common.py:44\u001b[0m, in \u001b[0;36mcreate_train_val_datasets\u001b[0;34m(datapath, train_config, val_config, test_config, load_data_func)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataclass is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_data_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_data_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m max_val \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mget_max_val()\n\u001b[1;32m     52\u001b[0m val_config\u001b[38;5;241m.\u001b[39mmax_val \u001b[38;5;241m=\u001b[39m max_val\n",
      "File \u001b[0;32m~/Projects_2/careamics/src/careamics/lvae_training/dataset/lc_dataset.py:61\u001b[0m, in \u001b[0;36mLCMultiChDloader.__init__\u001b[0;34m(self, data_config, fpath, load_data_fn, val_fraction, test_fraction)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     60\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (shape[\u001b[38;5;241m0\u001b[39m], shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, shape[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m---> 61\u001b[0m ds_data \u001b[38;5;241m=\u001b[39m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_shape\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scaled_data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# NOTE: These asserts are important. the resize method expects np.float32. otherwise, one gets weird results.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     66\u001b[0m     ds_data\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scaled_data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     67\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownsampled image should not have very different values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/localscratch/conda/envs/msr/lib/python3.10/site-packages/skimage/transform/_warps.py:203\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    200\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m    202\u001b[0m zoom_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m factors]\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mndi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndi_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m _clip_warp_output(image, out, mode, cval, clip)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/localscratch/conda/envs/msr/lib/python3.10/site-packages/scipy/ndimage/_interpolation.py:862\u001b[0m, in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[1;32m    858\u001b[0m zoom \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[1;32m    859\u001b[0m                  out\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64),\n\u001b[1;32m    860\u001b[0m                  where\u001b[38;5;241m=\u001b[39mzoom_div \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    861\u001b[0m zoom \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(zoom)\n\u001b[0;32m--> 862\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgrid_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setting up train, validation, and test data configs\n",
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=PATCH_SIZE,\n",
    "    num_channels=NUM_CHANNELS\n",
    ")\n",
    "\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = get_microsplit_parameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    img_size=PATCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    multiscale_count=3,\n",
    "    noise_model_path=NM_PATH,\n",
    "    target_channels=NUM_CHANNELS,\n",
    ")\n",
    "\n",
    "# create the dataset\n",
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA_PATH,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure `num_workers`\n",
    "In Windows and MacOS, setting `num_workers > 0` for dataloaders would cause out-of-memory issue and might crash the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_workers():\n",
    "    \"\"\"Utility function to set num_workers based on OS.\"\"\"\n",
    "    if platform.system() == \"Windows\" or platform.system() == \"Darwin\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 3  # or any other number suitable for your system\n",
    "\n",
    "experiment_params[\"num_workers\"] = get_num_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional:*** inspect data configurations and <nobr>Micro$\\mathbb{S}$plit</nobr> config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_show_configs = True\n",
    "\n",
    "if do_show_configs:\n",
    "    print(\"FYI: train_data_config\")\n",
    "    print(\"----------------------\")\n",
    "    for cfg in train_data_config:\n",
    "        print(cfg)\n",
    "\n",
    "    print(\"\\nFYI: experiment_params\")\n",
    "    print(\"----------------------\")\n",
    "    print(experiment_params)\n",
    "else:\n",
    "    print(\"You opted out of having all params printed... swiftly moving on... ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: create Dataloaders for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.2:** Prepare <nobr>Micro$\\mathbb{S}$plit</nobr> Training\n",
    "Next, we create all the configs for the upcoming network training run. These lines are not very intuitive and if you don't intend to dive really deep into CAREamics and the internals of <nobr>Micro$\\mathbb{S}$plit</nobr>, you might just execute these cells and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our data_stas known to the experiment we prepare\n",
    "experiment_params[\"data_stats\"] = data_stats\n",
    "\n",
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the <nobr>Micro$\\mathbb{S}$plit</nobr> model to be trained.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load checkpoint (optional and for you to implement)*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> If you would like to continue a previous training run or finetune a compatible pre-trained model, here would be a good place. You will need to figure out how to implement this for your use-case, but to give you a head-start, we left three potentially useful lines of code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from microsplit_reproducibility.notebook_utils.HT_LIF24 import load_pretrained_model\n",
    "# ckpt_path = load_checkpoint_path(f\"./pretrained_checkpoints/{EXPOSURE_DURATION}/\", best=True)\n",
    "# load_pretrained_model(model, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training data for a final check!\n",
    "***Tip:*** we show you a few samples of the prepared training data. In case you don't like what you see, execute the cell again and other randomly chosen patches will be shown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=NUM_CHANNELS, num_samples=3, patch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.3:** Train the prepared model!\n",
    "***Note:*** if this takes too long, there were to places above where we gave you options to *(i)* reduce the amount of training data, and *(ii)* chose to train for fewer epochs. Revisit your choices if you want to!\n",
    "\n",
    "***Note:*** Depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(\"./checkpoints/\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "# start the training\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.4:** Predict and visualize results for validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, reduce the validation dataset to speed up the evaluation\n",
    "val_dset.reduce_data([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note*** Parameter `mmse_count` is responsible for how many samples are generated for each patch. The default value is 1, but in this case you might see stitching artifacts because each patch will be slightly different. You can increase this value to 10 to get a smoother image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    get_unnormalized_predictions,\n",
    "    get_target,\n",
    "    get_input,\n",
    ")\n",
    "\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(\n",
    "    model,\n",
    "    val_dset,\n",
    "    data_key=val_dset._fpath.name,\n",
    "    mmse_count=experiment_params['mmse_count'],\n",
    "    num_workers=0,\n",
    "    batch_size=8\n",
    ")\n",
    "tar = get_target(val_dset)\n",
    "\n",
    "# get input as sum of the two channels\n",
    "inp = get_input(val_dset).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: visualize predictions on validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "ax[0, 0].imshow(tar[0, ..., 0], cmap=\"gray\")\n",
    "ax[0, 0].set_title(\"Input ch1\")\n",
    "ax[0, 1].imshow(tar[0, ..., 1], cmap=\"gray\")\n",
    "ax[0, 1].set_title(\"Input ch2\")\n",
    "ax[1, 0].imshow(stitched_predictions[0, ..., 0], cmap=\"gray\")\n",
    "ax[1, 0].set_title(\"Prediction ch1\")\n",
    "ax[1, 1].imshow(stitched_predictions[0, ..., 1], cmap=\"gray\")\n",
    "ax[1, 1].set_title(\"Prediction ch2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import full_frame_evaluation\n",
    "\n",
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds\"\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed view on some (foreground) locations...\n",
    "Below, we show few random foreground locations and the corresponding <nobr>Micro$\\mathbb{S}$plit</nobr> predictions.\n",
    "\n",
    "As before, also here you can execute the cell multiple times and different randomly chosen locations will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from microsplit_reproducibility.utils.utils import clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    pick_random_patches_with_content,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sz = 128\n",
    "rand_locations = pick_random_patches_with_content(tar, 128)\n",
    "h_start = rand_locations[\n",
    "    2, 0\n",
    "]  # np.random.randint(stitched_predictions.shape[1] - img_sz)\n",
    "w_start = rand_locations[\n",
    "    2, 1\n",
    "]  # np.random.randint(stitched_predictions.shape[2] - img_sz)\n",
    "\n",
    "ncols = 4 + 1\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i, 0].imshow(inp[0, h_start : h_start + img_sz, w_start : w_start + img_sz])\n",
    "    for j in range(ncols // 2):\n",
    "        vmin = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].min()\n",
    "        vmax = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].max()\n",
    "        ax[i, 2 * j + 1].imshow(\n",
    "            tar[0, h_start : h_start + img_sz, w_start : w_start + img_sz, j],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax[i, 2 * j + 2].imshow(\n",
    "            stitched_predictions[\n",
    "                0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "            ],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "\n",
    "ax[0, 0].set_title(\"Primary Input\")\n",
    "for i in range(2):\n",
    "    ax[0, 2 * i + 1].set_title(f\"Target Channel {i+1}\")\n",
    "    ax[0, 2 * i + 2].set_title(f\"Predicted Channel {i+1}\")\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* manual inspection of the predictions\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Task:</b> Set <i>y_start</i>, <i>x_start</i>, and <i>crop_size</i> to inspect the predictions at a  location of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_start = 600  # np.random.randint(stitched_predictions.shape[1] - crop_size)\n",
    "x_start = 1150  # np.random.randint(stitched_predictions.shape[2] - crop_size)\n",
    "crop_size = 128\n",
    "\n",
    "ncols = 3\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "ax[0, 0].imshow(inp[0, y_start : y_start + crop_size, x_start : x_start + crop_size])\n",
    "for i in range(ncols - 1):\n",
    "    vmin = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].min()\n",
    "    vmax = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].max()\n",
    "    ax[0, i + 1].imshow(\n",
    "        tar[0, y_start : y_start + crop_size, x_start : x_start + crop_size, i],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    ax[1, i + 1].imshow(\n",
    "        stitched_predictions[\n",
    "            0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "        ],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[1, 0].axis(\"off\")\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "ax[0, 1].set_title(\"Channel 1\")\n",
    "ax[0, 2].set_title(\"Channel 2\")\n",
    "# set y labels on the right for ax[0,2]\n",
    "ax[0, 2].yaxis.set_label_position(\"right\")\n",
    "ax[0, 2].set_ylabel(\"Target\")\n",
    "\n",
    "ax[1, 2].yaxis.set_label_position(\"right\")\n",
    "ax[1, 2].set_ylabel(\"Predicted\")\n",
    "\n",
    "print(\"Here the crop you selected:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Optional Step 1.4:*** Posterior Sampling\n",
    "For a given input patch, <nobr>Micro$\\mathbb{S}$plit</nobr> can generate multiple outputs. This is possible because <nobr>Micro$\\mathbb{S}$plit</nobr> is learning a full posterior of possible solutions, which is a quite powerful feature!\n",
    "\n",
    "As we elaborate in the <nobr>Micro$\\mathbb{S}$plit</nobr> paper and also later in the calibration notebook `03_calibration.ipynb`, this allows users to visually judge and even quantify the (data) uncertainty in the predictions their trained model makes.\n",
    "\n",
    "Below, we show two posterior samples and how much they differ for a few random foreground locations. Re-run the cell to see different randomly choosen locations and corresponding posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import show_sampling\n",
    "\n",
    "imgsz = 3\n",
    "ncols = 6\n",
    "examplecount = 3\n",
    "_, ax = plt.subplots(\n",
    "    figsize=(imgsz * ncols, imgsz * 2 * examplecount),\n",
    "    ncols=ncols,\n",
    "    nrows=2 * examplecount,\n",
    ")\n",
    "\n",
    "show_sampling(val_dset, model, ax=ax[:2])\n",
    "show_sampling(val_dset, model, ax=ax[2:4])\n",
    "show_sampling(val_dset, model, ax=ax[4:6])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done here! üëç Congratulations! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
